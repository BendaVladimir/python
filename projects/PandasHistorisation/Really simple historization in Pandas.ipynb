{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script was created an experiment if it is possible to use Pandas and default python tools to simulate historization process used to implement dimensional tables in datawarehouse. Maybe in some cases this is not a good way to implement process of historization, because of data that will grow exponentially, but as mentioned before this is just an experiment and worked quite well for used dataset. \n",
    "\n",
    "Data that I used is publically available list of eligible collateral at https://www.ecb.europa.eu/paym/coll/assets/html/list-MID.en.html and download full list of assets. You can download a list of eligible assets on daily bases a use Pandas to create historical view of eligible assets on daily basis. We would use Pandas for data manipulation and pickle the data after processing.\n",
    "\n",
    "Probably whole process could be made more real if the default values and changing datatypes of columns were implemented in script, but this is a thing for further development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing that will be necessary is to set a current date for processing CSV file. In case that this notebook would be implemented as a script than this variable would be passed as a parameter, when the script was run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_in = '20191007'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step input parameter will be converted to tuple of integers, which are then passed as parameter to date function. Variables current_date and end_date are used in historization process referencing validity of certain record. This will used in columns VALID_FROM and VALID_TO in Pandas dataframe.\n",
    "\n",
    "Someone might be tempted to use pandas.Timestamp as date format as we are using Pandas in this script. This would not be possible for variable end_date which we set value 9999-12-31. Maximal value of pandas.Timestamp is 2262-04-11 23:47:16.854775807 https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.max.html if you try to use this datatype, you will get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (int(dates_in[0:4]), int(dates_in[4:6]), int(dates_in[6:]))\n",
    "current_date = dt.date(f[0],f[1],f[2])\n",
    "end_date =  dt.date(9999, 12, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next line is going to be used to create list of columns in Pandas dataframe used in historization and listing columns used in creating HASH function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_df = ['ISIN_CODE', 'OTHER_REG_NUMBER', 'HAIRCUT_CATEGORY', 'TYPE',\n",
    "       'REFERENCE_MARKET', 'DENOMINATION', 'ISSUANCE_DATE', 'MATURITY_DATE',\n",
    "       'ISSUER_CSD', 'COUPON_RATE (%)', 'ISSUER_NAME', 'ISSUER_OTHER_NAME',\n",
    "       'ISSUER_RESIDENCE', 'ISSUER_GROUP', 'GUARANTOR_NAME',\n",
    "       'GUARANTOR_OTHER_NAME', 'GUARANTOR_RESIDENCE', 'GUARANTOR_GROUP',\n",
    "       'COUPON_DEFINITION', 'HAIRCUT', 'HAIRCUT_OWN_USE',\n",
    "       'OWN_USE_COVERED_BONDS','HASH','VALID_FROM','VALID_TO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function is used to create hash representation of a string that is passed as an input. Hashed output of an string that represents concatenation of all column values as string. This will return same value of hash for values already in dataframe if nothing has changed between different dates, but will create new hash sum if something has changed in values of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concanate_hash(input_string):\n",
    "\n",
    "    \"\"\"\n",
    "    Function is used to calculate hashed value of concatenated columns of records.\n",
    "\n",
    "    Input passed to this function is concatenated value of all columns in record into string. Function then uses sha256 algorithm to create hexadecimal output.\n",
    "\n",
    "    Parameters:\n",
    "    input_string (str): string containing concatenated value of string from all columns of record\n",
    "\n",
    "    Returns:\n",
    "    str: string representation of hashed function for all columns for record\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return hashlib.sha256(str(input_string).encode('utf-16')).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case that we have already used historization on this dataset, than the pickle file will exist in folder pickle_data, we will than load it into Pandas dataframe and remove pickle file.\n",
    "\n",
    "If we are running it for the first time, than empty dataframe will be created with columns which we have specified in variable columns_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./pickle_data/history.pickle'):\n",
    "    history_data = pd.read_pickle('./pickle_data/history.pickle')\n",
    "    os.remove('./pickle_data/history.pickle')\n",
    "else:\n",
    "    history_data = pd.DataFrame(columns=columns_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we will load stored CSV file into Pandas dataframe which will be used for historization. Data loaded into dataframe will hold current snapshot and additional columns will be added before historization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    temporary_data = pd.read_csv('./base_data/ea_csv_'+dates_in[2:]+'.csv', encoding=\"utf-16\", delimiter='\\t')\n",
    "except FileNotFoundError:\n",
    "    print('File does not exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the structure of CSV file has not changed. In case that file structure has changed, than we will not proceed with historization and we have to change whole process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if list(temporary_data.columns) != columns_df[:-3]:\n",
    "    raise Exception('There has been change in CSV file structure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we will add 3 new columns to the temporary dataframe. \n",
    "\n",
    "HASH columns will use all columns in temporary dataframe to calculate hashed output to new column. This will convert all columns to string, then it will concatenate them and at final step it will calculate hashed value that will be added to temporary dataframe.\n",
    "\n",
    "VALID_FROM column will contain date from which inserted value will be valid and inserted into historized dataframe.\n",
    "\n",
    "VALID_TO will contain maximal end date, which will be inserted into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary_data['HASH'] = temporary_data[columns_df[:-3]].astype(str).sum(axis=1).apply(concanate_hash)\n",
    "temporary_data['VALID_FROM'] = current_date\n",
    "temporary_data['VALID_TO'] = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create new dataframe that will create new records to be inserted into historized dataframe. This dataframe will contain only those records that are in temporary dataframe and are not included in current version of data in historized dataset.\n",
    "\n",
    "In order to filter out these records we will have to compare HASH columns between temporary dataset and historized dataset. \n",
    "\n",
    "From historized dataset we will select only currently valid records by selecting VALID_TO  equal to end_date. Then we will compare these records with records contained in temporary dataset with .isin() function, based on column HASH. And finally we will reverse this filter by using numpy logical_not in order to select those values that are not present in historized dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = temporary_data[np.logical_not(temporary_data['HASH'].isin(history_data[(history_data['VALID_TO'] == end_date)]['HASH']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will be used to create list of index values from historized dataset, that are not in temporary dataset. Index of values will be used to filter records from historized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = list(history_data[(history_data['VALID_TO'] == end_date) & (np.logical_not(history_data['HASH'].isin(temporary_data['HASH'])))].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use index list from previous cell to filter out records that will be updated and closed. Closing validity of record will be done by setting VALID_TO to current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_data.loc[index_list, 'VALID_TO'] = current_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending new records from tmp dataframe into historized dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_data = history_data.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new pickle file that will contain current version of historized dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_data.to_pickle('./pickle_data/history.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally print statement will show how many records are currently valid VALID_TO = 9999-12-31 and counts of values for other dates in VALID_TO column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999-12-31    24509\n",
      "2019-10-09      114\n",
      "2019-10-08       98\n",
      "Name: VALID_TO, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(history_data['VALID_TO'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
